To set up Apache Spark 2.4.5 to use Google Cloud Storage (GCS) with S3 HMAC keys and treat `https://storage.googleapis.com` as an S3 endpoint, you'll need to configure several components correctly. Here are the detailed steps and the necessary versions of AWS and Hadoop jars you'll need:

### Step-by-Step Setup

1. **Install Hadoop and AWS JARs**:
    - You need Hadoop 2.7.x jars and the AWS SDK for Java.
    - Download the required JAR files if not already included in your Spark/Hadoop distribution.

2. **Dependencies**:
    - **Hadoop**: You should use Hadoop version 2.7.x. The specific JARs you need include:
        - `hadoop-common-2.7.x.jar`
        - `hadoop-aws-2.7.x.jar`
    - **AWS SDK**: Use AWS SDK version 1.7.4. The specific JARs you need include:
        - `aws-java-sdk-1.7.4.jar`
        - `joda-time-2.9.9.jar` (or a compatible version)

3. **Configuration Files**:
    - **core-site.xml**: This file should be placed in the `$HADOOP_HOME/etc/hadoop/` directory and configured as follows:

    ```xml
    <configuration>
        <property>
            <name>fs.s3a.impl</name>
            <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
        </property>
        <property>
            <name>fs.s3a.access.key</name>
            <value>YOUR_ACCESS_KEY</value>
        </property>
        <property>
            <name>fs.s3a.secret.key</name>
            <value>YOUR_SECRET_KEY</value>
        </property>
        <property>
            <name>fs.s3a.endpoint</name>
            <value>storage.googleapis.com</value>
        </property>
        <property>
            <name>fs.s3a.path.style.access</name>
            <value>true</value>
        </property>
        <property>
            <name>fs.s3a.connection.ssl.enabled</name>
            <value>true</value>
        </property>
    </configuration>
    ```

4. **Running Spark**:
    - Ensure that your Spark configuration includes the Hadoop and AWS JARs in the classpath. You can add them to the `spark-submit` command using the `--jars` option:

    ```bash
    spark-submit --jars /path/to/hadoop-aws-2.7.x.jar,/path/to/aws-java-sdk-1.7.4.jar,/path/to/joda-time-2.9.9.jar --conf spark.hadoop.fs.s3a.endpoint=storage.googleapis.com your_spark_application.py
    ```

### Example Configuration in Python

Here's an example of how you might configure and use Spark to access GCS in a Python script:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("GCS_S3_Spark") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.access.key", "YOUR_ACCESS_KEY") \
    .config("spark.hadoop.fs.s3a.secret.key", "YOUR_SECRET_KEY") \
    .config("spark.hadoop.fs.s3a.endpoint", "storage.googleapis.com") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true") \
    .getOrCreate()

# Example to read a file from GCS
df = spark.read.text("s3a://your-bucket-name/path/to/your/file.txt")
df.show()
```

### Summary of Required JARs

- Hadoop Common: `hadoop-common-2.7.x.jar`
- Hadoop AWS: `hadoop-aws-2.7.x.jar`
- AWS SDK: `aws-java-sdk-1.7.4.jar`
- Joda-Time: `joda-time-2.9.9.jar`

By following these steps and using the specified versions of the JARs, you should be able to configure Spark 2.4.5 to use GCS with S3 HMAC keys correctly.

Yes, this setup can connect to the HTTPS version of `storage.googleapis.com` endpoint. The configuration and the JAR versions specified support HTTPS connections to Google Cloud Storage (GCS) using the S3A connector.

### Key Points to Ensure HTTPS Connection

1. **Hadoop Configuration**: Make sure the `core-site.xml` is correctly configured to enable SSL.

    ```xml
    <configuration>
        <property>
            <name>fs.s3a.impl</name>
            <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
        </property>
        <property>
            <name>fs.s3a.access.key</name>
            <value>YOUR_ACCESS_KEY</value>
        </property>
        <property>
            <name>fs.s3a.secret.key</name>
            <value>YOUR_SECRET_KEY</value>
        </property>
        <property>
            <name>fs.s3a.endpoint</name>
            <value>storage.googleapis.com</value>
        </property>
        <property>
            <name>fs.s3a.path.style.access</name>
            <value>true</value>
        </property>
        <property>
            <name>fs.s3a.connection.ssl.enabled</name>
            <value>true</value>
        </property>
    </configuration>
    ```

2. **Spark Configuration**: Ensure that you pass the necessary configurations when submitting your Spark job.

    ```bash
    spark-submit --jars /path/to/hadoop-aws-2.7.x.jar,/path/to/aws-java-sdk-1.7.4.jar,/path/to/joda-time-2.9.9.jar --conf spark.hadoop.fs.s3a.endpoint=storage.googleapis.com --conf spark.hadoop.fs.s3a.connection.ssl.enabled=true your_spark_application.py
    ```

3. **Dependencies**:
    - **Hadoop Common**: `hadoop-common-2.7.x.jar`
    - **Hadoop AWS**: `hadoop-aws-2.7.x.jar`
    - **AWS SDK**: `aws-java-sdk-1.7.4.jar`
    - **Joda-Time**: `joda-time-2.9.9.jar`

### Example Spark Application Configuration

Here is an example of configuring and using Spark to connect to GCS using HTTPS:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("GCS_S3_Spark") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.access.key", "YOUR_ACCESS_KEY") \
    .config("spark.hadoop.fs.s3a.secret.key", "YOUR_SECRET_KEY") \
    .config("spark.hadoop.fs.s3a.endpoint", "storage.googleapis.com") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true") \
    .getOrCreate()

# Example to read a file from GCS
df = spark.read.text("s3a://your-bucket-name/path/to/your/file.txt")
df.show()
```

By setting `fs.s3a.connection.ssl.enabled` to `true`, the connector will use HTTPS to communicate with `storage.googleapis.com`. This ensures a secure connection between your Spark application and Google Cloud Storage. 

With these configurations, you should be able to connect to the HTTPS version of the `storage.googleapis.com` endpoint using the specified versions of Spark, Hadoop, and AWS SDK JARs.

To download all the necessary JAR files for configuring Apache Spark to use Google Cloud Storage (GCS) with S3 HMAC keys, you can follow these steps:

### 1. **Apache Hadoop JARs**
You can download the Hadoop JARs from the Apache Hadoop official site or Maven Central Repository.

- **Hadoop Common JAR** (`hadoop-common-2.7.x.jar`):
  - Visit the [Apache Hadoop releases page](https://hadoop.apache.org/releases.html).
  - Download the binary distribution of Hadoop 2.7.x.
  - The JAR can be found in the `share/hadoop/common/` directory after extracting the archive.

- **Hadoop AWS JAR** (`hadoop-aws-2.7.x.jar`):
  - Available from Maven Central.
  - Direct download link for `hadoop-aws-2.7.4.jar`: [Maven Repository](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.4/hadoop-aws-2.7.4.jar).

### 2. **AWS SDK JARs**
You can download the AWS SDK JARs from the Maven Central Repository.

- **AWS SDK for Java** (`aws-java-sdk-1.7.4.jar`):
  - Direct download link for `aws-java-sdk-1.7.4.jar`: [Maven Repository](https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar).

- **Joda-Time JAR** (`joda-time-2.9.9.jar`):
  - Direct download link for `joda-time-2.9.9.jar`: [Maven Repository](https://repo1.maven.org/maven2/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar).

### Download Links Summary
- **Hadoop Common**: [Download from Apache Hadoop](https://hadoop.apache.org/releases.html) or use a specific version from Maven:
  - [hadoop-common-2.7.4.jar](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.7.4/hadoop-common-2.7.4.jar)
- **Hadoop AWS**: [hadoop-aws-2.7.4.jar](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.4/hadoop-aws-2.7.4.jar)
- **AWS SDK**: [aws-java-sdk-1.7.4.jar](https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar)
- **Joda-Time**: [joda-time-2.9.9.jar](https://repo1.maven.org/maven2/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar)

### Alternative: Using Maven or Gradle

If you are using Maven or Gradle to manage dependencies, you can include these dependencies in your `pom.xml` or `build.gradle` file. This way, the build tool will automatically download and include the necessary JARs.

**Maven**:
```xml
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>2.7.4</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-aws</artifactId>
        <version>2.7.4</version>
    </dependency>
    <dependency>
        <groupId>com.amazonaws</groupId>
        <artifactId>aws-java-sdk</artifactId>
        <version>1.7.4</version>
    </dependency>
    <dependency>
        <groupId>joda-time</groupId>
        <artifactId>joda-time</artifactId>
        <version>2.9.9</version>
    </dependency>
</dependencies>
```

**Gradle**:
```groovy
dependencies {
    implementation 'org.apache.hadoop:hadoop-common:2.7.4'
    implementation 'org.apache.hadoop:hadoop-aws:2.7.4'
    implementation 'com.amazonaws:aws-java-sdk:1.7.4'
    implementation 'joda-time:joda-time:2.9.9'
}
```

Using these methods, you can ensure that all required dependencies are downloaded and included in your project automatically.


Setting up Apache Spark 2.4.5 with Hadoop 2.7.x to use Google Cloud Storage (GCS) with S3 HMAC keys can be challenging due to potential compatibility issues and the complexity of configurations. Here are some considerations and an alternative approach to ensure full compatibility.

### Compatibility Considerations

1. **Hadoop Compatibility**: The Hadoop versions compatible with GCS and S3A configurations can vary. Hadoop 2.7.x might not have full support for the latest GCS features or the required stability for HMAC keys.

2. **AWS SDK Versions**: AWS SDK versions should be carefully selected. Older versions might not support all the features required for seamless integration with GCS using S3A.

### Suggested Approach for GCS Integration

Instead of directly using Hadoop 2.7.x and AWS SDK 1.7.4, consider the following approach to ensure better compatibility and support:

1. **Use Hadoop 3.x**: Hadoop 3.x has improved support for cloud storage and S3A configurations. It includes bug fixes and features not available in Hadoop 2.7.x.

2. **Updated AWS SDK**: Use a more recent version of the AWS SDK to ensure compatibility and support for the latest features.

### Step-by-Step Setup

#### 1. Download Hadoop 3.x and AWS SDK JARs

- **Hadoop Common JAR** (`hadoop-common-3.x.x.jar`):
  - Visit the [Apache Hadoop releases page](https://hadoop.apache.org/releases.html).
  - Download the binary distribution of Hadoop 3.x.
  - The JAR can be found in the `share/hadoop/common/` directory after extracting the archive.

- **Hadoop AWS JAR** (`hadoop-aws-3.x.x.jar`):
  - Available from Maven Central.
  - Direct download link for `hadoop-aws-3.2.1.jar`: [Maven Repository](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.1/hadoop-aws-3.2.1.jar).

- **AWS SDK for Java** (`aws-java-sdk-bundle-1.11.375.jar`):
  - Direct download link for `aws-java-sdk-bundle-1.11.375.jar`: [Maven Repository](https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.375/aws-java-sdk-bundle-1.11.375.jar).

#### 2. Configure `core-site.xml`

Place the configuration file in the `$HADOOP_HOME/etc/hadoop/` directory:

```xml
<configuration>
    <property>
        <name>fs.s3a.impl</name>
        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
    </property>
    <property>
        <name>fs.s3a.access.key</name>
        <value>YOUR_ACCESS_KEY</value>
    </property>
    <property>
        <name>fs.s3a.secret.key</name>
        <value>YOUR_SECRET_KEY</value>
    </property>
    <property>
        <name>fs.s3a.endpoint</name>
        <value>storage.googleapis.com</value>
    </property>
    <property>
        <name>fs.s3a.path.style.access</name>
        <value>true</value>
    </property>
    <property>
        <name>fs.s3a.connection.ssl.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>fs.s3a.connection.maximum</name>
        <value>500</value>
    </property>
    <property>
        <name>fs.s3a.aws.credentials.provider</name>
        <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>
    </property>
</configuration>
```

#### 3. Running Spark

Ensure that your Spark configuration includes the Hadoop and AWS JARs in the classpath. You can add them to the `spark-submit` command using the `--jars` option:

```bash
spark-submit --jars /path/to/hadoop-aws-3.2.1.jar,/path/to/aws-java-sdk-bundle-1.11.375.jar --conf spark.hadoop.fs.s3a.endpoint=storage.googleapis.com --conf spark.hadoop.fs.s3a.connection.ssl.enabled=true your_spark_application.py
```

### Example Spark Application Configuration

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("GCS_S3_Spark") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.access.key", "YOUR_ACCESS_KEY") \
    .config("spark.hadoop.fs.s3a.secret.key", "YOUR_SECRET_KEY") \
    .config("spark.hadoop.fs.s3a.endpoint", "storage.googleapis.com") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true") \
    .getOrCreate()

# Example to read a file from GCS
df = spark.read.text("s3a://your-bucket-name/path/to/your/file.txt")
df.show()
```

### Summary of Required JARs for Hadoop 3.x

- **Hadoop Common**: `hadoop-common-3.2.1.jar`
- **Hadoop AWS**: `hadoop-aws-3.2.1.jar`
- **AWS SDK**: `aws-java-sdk-bundle-1.11.375.jar`

By using Hadoop 3.x and updated AWS SDK versions, you can ensure better compatibility and support for integrating Spark with GCS using S3 HMAC keys.


Using Hadoop 3.x jars with Spark 2.4.5 can be challenging due to potential compatibility issues. Spark 2.4.5 is typically built and tested with Hadoop 2.7.x, so there might be issues when mixing with Hadoop 3.x libraries. However, many users have successfully used Hadoop 3.x with Spark 2.4.x by carefully managing dependencies and configurations.

### Recommended Approach

To avoid potential issues, it's recommended to test this setup in a controlled environment before deploying it in production. Below are the steps and configurations to help ensure compatibility:

### Step-by-Step Setup

#### 1. Download the Required JARs

- **Hadoop Common JAR**: `hadoop-common-3.2.1.jar`
- **Hadoop AWS JAR**: `hadoop-aws-3.2.1.jar`
- **AWS SDK for Java**: `aws-java-sdk-bundle-1.11.375.jar`

You can download these JARs from the Maven Repository:
- [hadoop-common-3.2.1.jar](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.2.1/hadoop-common-3.2.1.jar)
- [hadoop-aws-3.2.1.jar](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.1/hadoop-aws-3.2.1.jar)
- [aws-java-sdk-bundle-1.11.375.jar](https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.375/aws-java-sdk-bundle-1.11.375.jar)

#### 2. Configure `core-site.xml`

Place the configuration file in the `$HADOOP_HOME/etc/hadoop/` directory:

```xml
<configuration>
    <property>
        <name>fs.s3a.impl</name>
        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
    </property>
    <property>
        <name>fs.s3a.access.key</name>
        <value>YOUR_ACCESS_KEY</value>
    </property>
    <property>
        <name>fs.s3a.secret.key</name>
        <value>YOUR_SECRET_KEY</value>
    </property>
    <property>
        <name>fs.s3a.endpoint</name>
        <value>storage.googleapis.com</value>
    </property>
    <property>
        <name>fs.s3a.path.style.access</name>
        <value>true</value>
    </property>
    <property>
        <name>fs.s3a.connection.ssl.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>fs.s3a.connection.maximum</name>
        <value>500</value>
    </property>
    <property>
        <name>fs.s3a.aws.credentials.provider</name>
        <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>
    </property>
</configuration>
```

#### 3. Running Spark

Ensure that your Spark configuration includes the Hadoop and AWS JARs in the classpath. You can add them to the `spark-submit` command using the `--jars` option:

```bash
spark-submit --jars /path/to/hadoop-common-3.2.1.jar,/path/to/hadoop-aws-3.2.1.jar,/path/to/aws-java-sdk-bundle-1.11.375.jar --conf spark.hadoop.fs.s3a.endpoint=storage.googleapis.com --conf spark.hadoop.fs.s3a.connection.ssl.enabled=true your_spark_application.py
```

### Example Spark Application Configuration

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("GCS_S3_Spark") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.access.key", "YOUR_ACCESS_KEY") \
    .config("spark.hadoop.fs.s3a.secret.key", "YOUR_SECRET_KEY") \
    .config("spark.hadoop.fs.s3a.endpoint", "storage.googleapis.com") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true") \
    .getOrCreate()

# Example to read a file from GCS
df = spark.read.text("s3a://your-bucket-name/path/to/your/file.txt")
df.show()
```

### Alternative: Using Hadoop 2.8.x

If you encounter issues with Hadoop 3.x, another approach is to use Hadoop 2.8.x, which has better support for cloud storage compared to 2.7.x and fewer compatibility issues with Spark 2.4.5.

#### Download Hadoop 2.8.x JARs

- **Hadoop Common JAR**: `hadoop-common-2.8.5.jar`
- **Hadoop AWS JAR**: `hadoop-aws-2.8.5.jar`

#### Example Download Links:

- [hadoop-common-2.8.5.jar](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.8.5/hadoop-common-2.8.5.jar)
- [hadoop-aws-2.8.5.jar](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.8.5/hadoop-aws-2.8.5.jar)

### Summary

Using Hadoop 3.x jars with Spark 2.4.5 is possible but requires careful testing to ensure compatibility. Alternatively, using Hadoop 2.8.x can provide a more stable solution. Download the required JARs, configure the `core-site.xml` correctly, and include the JARs in your `spark-submit` command to set up your environment.


Using a Google Cloud Storage (GCS) JSON key file with Apache Spark 2.4.5 is a common approach for authentication. This method leverages the Google Cloud Storage connector for Hadoop, which simplifies the integration with Spark. Below are the steps to configure Spark 2.4.5 to use a GCS JSON key file for accessing GCS.

### Step-by-Step Setup

#### 1. Download the Required JARs

- **Google Cloud Storage Connector for Hadoop**:
  - Download the GCS connector JAR compatible with Hadoop 2.x. For example, `gcs-connector-hadoop2-latest.jar`:
    - [Download Link](https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar)

- **Other Dependencies**:
  - Ensure you have the necessary Google API client libraries. However, these are usually bundled with the GCS connector.

#### 2. Set Up Your GCS Credentials

1. **Create a Service Account and Download JSON Key**:
   - Go to the Google Cloud Console.
   - Navigate to `IAM & Admin` > `Service Accounts`.
   - Create a new service account or use an existing one.
   - Create a key in JSON format and download it.

2. **Set the Environment Variable**:
   - Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to the location of your JSON key file.
   
   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your-service-account-key.json"
   ```

#### 3. Configure `core-site.xml`

Create or modify the `core-site.xml` file, typically located in the `$HADOOP_HOME/etc/hadoop/` directory:

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>gs://your-gcs-bucket</value>
    </property>
    <property>
        <name>fs.gs.impl</name>
        <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
    </property>
    <property>
        <name>google.cloud.auth.service.account.enable</name>
        <value>true</value>
    </property>
    <property>
        <name>google.cloud.auth.service.account.json.keyfile</name>
        <value>/path/to/your-service-account-key.json</value>
    </property>
</configuration>
```

#### 4. Running Spark

Ensure that your Spark configuration includes the GCS connector JAR in the classpath. You can add it to the `spark-submit` command using the `--jars` option:

```bash
spark-submit --jars /path/to/gcs-connector-hadoop2-latest.jar --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/path/to/your-service-account-key.json your_spark_application.py
```

### Example Spark Application Configuration

Here's how you might configure and use Spark to access GCS in a Python script:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("GCS_Spark") \
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", "/path/to/your-service-account-key.json") \
    .getOrCreate()

# Example to read a file from GCS
df = spark.read.text("gs://your-gcs-bucket/path/to/your/file.txt")
df.show()
```

### Summary of Required JARs

- **GCS Connector**: `gcs-connector-hadoop2-latest.jar`

By following these steps and using the GCS JSON key file for authentication, you can configure Spark 2.4.5 to access Google Cloud Storage directly. This method avoids the complexities of using S3 HMAC keys and provides a more straightforward and secure way to connect to GCS.


Yes, using the Google Cloud Storage (GCS) JSON key file with Apache Spark 2.4.5 and Hadoop 2.x jars is fully compatible. The GCS connector is designed to work seamlessly with Hadoop 2.x, which includes versions like 2.7.x that Spark 2.4.5 is typically built and tested against.

Here's a more detailed guide to ensure compatibility:

### Step-by-Step Setup

#### 1. Download the Required JARs

- **Google Cloud Storage Connector for Hadoop**:
  - Download the GCS connector JAR compatible with Hadoop 2.x. For example, `gcs-connector-hadoop2-latest.jar`:
    - [Download Link](https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar)

- **Hadoop Common JAR**:
  - Download `hadoop-common-2.7.x.jar` from the Apache Hadoop releases page or Maven Central.
    - Example: [hadoop-common-2.7.4.jar](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.7.4/hadoop-common-2.7.4.jar)

- **Hadoop AWS JAR**:
  - Download `hadoop-aws-2.7.x.jar` if you need it for other purposes, though it's not required for GCS.
    - Example: [hadoop-aws-2.7.4.jar](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.4/hadoop-aws-2.7.4.jar)

#### 2. Set Up Your GCS Credentials

1. **Create a Service Account and Download JSON Key**:
   - Go to the Google Cloud Console.
   - Navigate to `IAM & Admin` > `Service Accounts`.
   - Create a new service account or use an existing one.
   - Create a key in JSON format and download it.

2. **Set the Environment Variable**:
   - Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to the location of your JSON key file.
   
   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your-service-account-key.json"
   ```

#### 3. Configure `core-site.xml`

Create or modify the `core-site.xml` file, typically located in the `$HADOOP_HOME/etc/hadoop/` directory:

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>gs://your-gcs-bucket</value>
    </property>
    <property>
        <name>fs.gs.impl</name>
        <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
    </property>
    <property>
        <name>google.cloud.auth.service.account.enable</name>
        <value>true</value>
    </property>
    <property>
        <name>google.cloud.auth.service.account.json.keyfile</name>
        <value>/path/to/your-service-account-key.json</value>
    </property>
</configuration>
```

#### 4. Running Spark

Ensure that your Spark configuration includes the GCS connector JAR in the classpath. You can add it to the `spark-submit` command using the `--jars` option:

```bash
spark-submit --jars /path/to/gcs-connector-hadoop2-latest.jar --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/path/to/your-service-account-key.json your_spark_application.py
```

### Example Spark Application Configuration

Here's how you might configure and use Spark to access GCS in a Python script:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("GCS_Spark") \
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", "/path/to/your-service-account-key.json") \
    .getOrCreate()

# Example to read a file from GCS
df = spark.read.text("gs://your-gcs-bucket/path/to/your/file.txt")
df.show()
```

### Summary

- **GCS Connector JAR**: `gcs-connector-hadoop2-latest.jar`
- **Hadoop Common JAR**: `hadoop-common-2.7.x.jar`

By following these steps and configurations, you ensure that Spark 2.4.5 can access GCS using a service account JSON key file. This setup is fully compatible with Hadoop 2.x jars, providing a straightforward and secure way to connect to GCS.


Using a GCS JSON key file with Spark 2.4.5 should not affect other Spark functionalities, such as the Spark History Server, default file system settings, `spark-defaults.conf`, and other Spark configurations. The GCS integration mainly involves configuring Hadoop to use the Google Cloud Storage connector for accessing GCS buckets, and this should work seamlessly alongside other Spark features. Here's how you can set it up:

### Step-by-Step Setup

#### 1. Download the Required JARs

- **Google Cloud Storage Connector for Hadoop**:
  - Download the GCS connector JAR compatible with Hadoop 2.x:
    - [Download Link](https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar)

- **Hadoop Common JAR**:
  - Example: [hadoop-common-2.7.4.jar](https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.7.4/hadoop-common-2.7.4.jar)

#### 2. Set Up Your GCS Credentials

1. **Create a Service Account and Download JSON Key**:
   - Go to the Google Cloud Console.
   - Navigate to `IAM & Admin` > `Service Accounts`.
   - Create a new service account or use an existing one.
   - Create a key in JSON format and download it.

2. **Set the Environment Variable**:
   - Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to the location of your JSON key file.
   
   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your-service-account-key.json"
   ```

#### 3. Configure `core-site.xml`

Place the configuration file in the `$HADOOP_HOME/etc/hadoop/` directory:

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>gs://your-gcs-bucket</value>
    </property>
    <property>
        <name>fs.gs.impl</name>
        <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
    </property>
    <property>
        <name>google.cloud.auth.service.account.enable</name>
        <value>true</value>
    </property>
    <property>
        <name>google.cloud.auth.service.account.json.keyfile</name>
        <value>/path/to/your-service-account-key.json</value>
    </property>
</configuration>
```

#### 4. Configure `spark-defaults.conf`

Ensure your `spark-defaults.conf` file is set up to include any necessary GCS settings. This file is typically located in `$SPARK_HOME/conf/`.

Add the following configurations:

```properties
spark.hadoop.fs.gs.impl com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
spark.hadoop.google.cloud.auth.service.account.enable true
spark.hadoop.google.cloud.auth.service.account.json.keyfile /path/to/your-service-account-key.json
```

#### 5. Running Spark

Ensure that your Spark configuration includes the GCS connector JAR in the classpath. You can add it to the `spark-submit` command using the `--jars` option:

```bash
spark-submit --jars /path/to/gcs-connector-hadoop2-latest.jar --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/path/to/your-service-account-key.json your_spark_application.py
```

### Example Spark Application Configuration

Here's how you might configure and use Spark to access GCS in a Python script:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("GCS_Spark") \
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", "/path/to/your-service-account-key.json") \
    .getOrCreate()

# Example to read a file from GCS
df = spark.read.text("gs://your-gcs-bucket/path/to/your/file.txt")
df.show()
```

### Spark History Server Configuration

Ensure that your History Server configuration in `spark-defaults.conf` is correctly set to use a path accessible by the History Server. If you want to use GCS for storing event logs, you can specify the GCS bucket path:

```properties
spark.eventLog.enabled true
spark.eventLog.dir gs://your-gcs-bucket/spark-history
```

### Summary

- **GCS Connector JAR**: `gcs-connector-hadoop2-latest.jar`
- **Hadoop Common JAR**: `hadoop-common-2.7.x.jar`
- **Configurations in `core-site.xml` and `spark-defaults.conf`**: Ensure the correct settings for GCS.

By following these steps, you ensure that Spark 2.4.5 can access Google Cloud Storage using a service account JSON key file while maintaining full compatibility with other Spark functionalities such as the History Server and other default configurations. This setup leverages the GCS connector, which is designed to work seamlessly with Hadoop 2.x and, by extension, Spark 2.4.5.
